{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AtariBreakout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Boyz6e5ipDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhs1GdFeihuF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e94b3b41-3864-43f8-a684-a6027abe8a43"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"Breakout-ramDeterministic-v4\")\n",
        "\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(128,)\n",
            "Action space: Discrete(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q-VzG7KuQBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap=200000\n",
        "batch_size=32\n",
        "lr=1e-5\n",
        "eps=1\n",
        "n_ep=20000\n",
        "gamma=0.99\n",
        "no_of_actions=4\n",
        "state_size=128\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wves258PvqvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = random.sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urlzjV1Jv_Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "            m.bias.data.fill_(0.01)\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kkET3kEPqQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotter(s,p):\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        plt.plot(np.arange(len(s)), s)\n",
        "        plt.ylabel('av_score')\n",
        "        plt.xlabel('Episode*50')\n",
        "        plt.show()\n",
        "\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        plt.plot(np.arange(len(p)), p)\n",
        "        plt.ylabel('q_values')\n",
        "        plt.xlabel('episode*20')\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNMd8Wg2P4ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model():\n",
        "        model_save_name = 'breakout1.pt'\n",
        "        path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "        checkpoint = {\n",
        "                        'model_state_dict':agent.model.state_dict(),\n",
        "                        'target_state_dict':agent.target.state_dict(),\n",
        "                        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "                        'memory':agent.memory\n",
        "                        }\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "def save_model2():\n",
        "        model_save_name = 'breakout2.pt'\n",
        "        path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "        checkpoint = {\n",
        "                        'model_state_dict':agent.model.state_dict(),\n",
        "                        'target_state_dict':agent.target.state_dict(),\n",
        "                        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "                        'memory':agent.memory\n",
        "                        }\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "def save_agent():\n",
        "        model_save_name = 'breakout_agent.pt'\n",
        "        path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "        checkpoint = {\n",
        "                        'n':n,\n",
        "                        'i':i,\n",
        "                        'eps': eps,\n",
        "                        's':s,\n",
        "                        'p':p,\n",
        "                        'av':scores\n",
        "                        }\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "def load_agent():\n",
        "      model_save_name = 'breakout_agent.pt'\n",
        "      path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "      checkpoint=torch.load(path)\n",
        "      n=checkpoint['n']\n",
        "      print(n)\n",
        "      i=checkpoint['i']\n",
        "      s=checkpoint['s']\n",
        "      p=checkpoint['p']\n",
        "      scores=checkpoint['av']\n",
        "      eps=checkpoint['eps']\n",
        "\n",
        "def load_tests():\n",
        "    model_save_name = 'breakout_test.pt'\n",
        "    per=torch.load(F\"/content/gdrive/My Drive/{model_save_name}\" )\n",
        "    per=(per['test_states'])\n",
        "    test_states= np.zeros((20000,512))\n",
        "    test_states=np.concatenate(per, out=test_states)\n",
        "    test_states=torch.from_numpy(test_states).float()\n",
        "    return test_states\n",
        "def save_tests():\n",
        "      model_save_name = 'breakout_test.pt'\n",
        "      path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "      checkpoint = {\n",
        "                      'test_states':test_states\n",
        "                      }\n",
        "\n",
        "      torch.save(checkpoint, path)\n",
        "def load_model():\n",
        "      model_save_name = 'breakout1.pt'\n",
        "      path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "      checkpoint=torch.load(path)\n",
        "      agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      agent.target.load_state_dict(checkpoint['target_state_dict'])\n",
        "      agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      agent.memory=(checkpoint['memory'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l50okTtBejYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class dqn(nn.Module):\n",
        "    def __init__(self,state_size,no_of_actions):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(128*4, 256)\n",
        "        self.fc2=nn.Linear(256,128)\n",
        "        self.fc3=nn.Linear(128,64)\n",
        "        self.fc4=nn.Linear(128,64)\n",
        "        self.fc5 = nn.Linear(64, no_of_actions)\n",
        "        self.fc6 = nn.Linear(64 , 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out=F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out=F.relu(out)\n",
        "        V = F.relu(self.fc3(out))\n",
        "        v =self.fc6(V)\n",
        "        A=F.relu(self.fc4(out))\n",
        "        a = self.fc5(A)\n",
        "        q=v+(a-torch.mean(a, dim=1, keepdim=True))\n",
        "        return q\n",
        "    \n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcqRBCTovrnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class agent:\n",
        "    def __init__(self):\n",
        "    \n",
        "        self.memory=ReplayMemory(cap)\n",
        "        self.model=dqn(state_size,no_of_actions)\n",
        "        self.target=dqn(state_size,no_of_actions)\n",
        "        self.target.apply(init_weights)\n",
        "        self.model.apply(init_weights)\n",
        "        self.optimizer=optim.Adam(self.model.parameters(),lr)\n",
        "        self.x=0\n",
        "      \n",
        "    \n",
        "    def remember(self,state,action,reward,next_state,done):\n",
        "      \n",
        "        self.memory.append([state,action,reward,next_state,done])\n",
        "        \n",
        "    def sample(self):\n",
        "        \n",
        "        experiences=self.memory.sample(batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float()\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long()\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float()\n",
        "        next_states =  torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float()\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float()\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "    \n",
        "   \n",
        "    def train(self):\n",
        "        states, actions, rewards, next_states, dones = self.sample()\n",
        "        Q=self.model(states).gather(1, actions)\n",
        "        _,a=torch.max(self.model(next_states),1,keepdim=True)\n",
        "        Q_next_state=self.target(next_states).gather(1, a)\n",
        "        Q_target=rewards+gamma*(Q_next_state*(1-dones))\n",
        "        loss=F.mse_loss(Q,Q_target)\n",
        "        self.x+=1\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        if(self.x%10000==0):\n",
        "            self.update(self.model,self.target)\n",
        "\n",
        "   \n",
        "    def action(self,state,eps):\n",
        "       \n",
        "        if random.random() > eps:\n",
        "            with torch.no_grad():\n",
        "                state= torch.from_numpy(state).float()\n",
        "                action_values = self.model(state)\n",
        "            action= (np.argmax(action_values).item())\n",
        "        else:\n",
        "            action= random.choice(np.arange(no_of_actions))\n",
        "        return action\n",
        "    \n",
        "    def update(self,local,target):\n",
        "         for target_param, local_param in zip(target.parameters(), local.parameters()):\n",
        "            target_param.data.copy_(local_param.data )\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1mN9iE4PJ_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0ef32d7b-2c1c-4827-9e11-d69cbcefc1f8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlGm7rC7WHFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent=agent()\n",
        "summary(agent.model,(1,512))\n",
        "test_states=load_tests()\n",
        "scores=deque(maxlen=100)\n",
        "i=0\n",
        "s=[]\n",
        "p=[]\n",
        "\n",
        "for n in range(0,n_ep):\n",
        "    h=0\n",
        "    \n",
        "    score=0\n",
        "    state=np.zeros((1,3*128))\n",
        "    obs=env.reset()\n",
        "    obs=obs.reshape(1,128)\n",
        "    state=np.concatenate((state,obs/255),axis=1)\n",
        "    done=False\n",
        "    li=5\n",
        "    while True:\n",
        "        if(i<50000):\n",
        "          eps=1\n",
        "        elif(i<1050000):\n",
        "          eps-=0.9/1000000\n",
        "        elif(i<2050000):\n",
        "          eps-=0.05/1000000\n",
        "        action= agent.action( state, eps)\n",
        "        next_obs,reward,done,_=env.step(action)\n",
        "        lf=env.ale.lives()\n",
        "        score+=reward\n",
        "        next_obs=next_obs.reshape(1,128)\n",
        "        if(li==lf):\n",
        "            next_state=state[:,128:].copy()\n",
        "            next_state=np.concatenate([next_state,next_obs/255.0], axis = 1)\n",
        "            agent.remember(state,action,reward,next_state,done)\n",
        "        else:\n",
        "            li=lf\n",
        "            next_state=np.zeros((1,3*128))\n",
        "            next_state=np.concatenate([next_state,next_obs/255.0], axis = 1)\n",
        "            agent.remember(state,action,reward,next_state,1)\n",
        "        \n",
        "        i+=1\n",
        "\n",
        "        state=next_state.copy()\n",
        "        h+=1\n",
        "        if((i+1)>batch_size):\n",
        "            agent.train()\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if((n+1)%20==0):\n",
        "        with torch.no_grad():\n",
        "            v=(torch.max(agent.model(test_states),1,keepdim=True)).values\n",
        "        p.append(v.mean())\n",
        "\n",
        "    scores.append(score)\n",
        "    avg_reward = np.mean(scores)\n",
        "    print('Game {}, Score = {},Steps = {}, av_score={}'.format(n+1,score,h,avg_reward))\n",
        "    if((n+1)%50==0):\n",
        "        s.append(avg_reward)\n",
        "    if((n+1)%300==0): \n",
        "        print(i,eps)\n",
        "        save_agent()\n",
        "        if((n+1)%600==0):\n",
        "            save_model()\n",
        "        else:\n",
        "            save_model2()\n",
        "        plotter(s,p)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsSsyRbSa9vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def record(out):\n",
        "    image = env.unwrapped._get_image()\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    out.write(image)\n",
        "    return out\n",
        "    \n",
        "    \n",
        "k=[]\n",
        "eps=0.05\n",
        "\n",
        "for n in range(0,20):\n",
        "    h=0\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter('/content/gdrive/My Drive/breakout' + str(_) + '.avi',fourcc,20, (160, 210))\n",
        "    score=0\n",
        "    state=np.zeros((1,3*128))\n",
        "    obs=env.reset()\n",
        "    out = record(out)\n",
        "    obs=obs.reshape(1,128)\n",
        "    state=np.concatenate((state,obs/255),axis=1)\n",
        "    done=False\n",
        "    li=5\n",
        "    while True:\n",
        "        action= agent.action( state, eps)\n",
        "        next_obs,reward,done,_=env.step(action)\n",
        "        out = record(out)\n",
        "        lf=env.ale.lives()\n",
        "        score+=reward\n",
        "        next_obs=next_obs.reshape(1,128)\n",
        "        if(li==lf):\n",
        "            next_state=state[:,128:].copy()\n",
        "            next_state=np.concatenate([next_state,next_obs/255.0], axis = 1)\n",
        "            \n",
        "        else:\n",
        "            li=lf\n",
        "            next_state=np.zeros((1,3*128))\n",
        "            next_state=np.concatenate([next_state,next_obs/255.0], axis = 1)\n",
        "            \n",
        "        \n",
        "       \n",
        "\n",
        "        state=next_state.copy()\n",
        "        h+=1\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "   \n",
        "\n",
        "    k.append(score)\n",
        "    avg_reward = np.mean(k)\n",
        "    print('Game {}, Score = {},Steps = {}, av_score={}'.format(n+1,score,h,avg_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXDyeb9Si-P0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLSmT6yfUY4I",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}